{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"External evaluation of the school and academic achievements in relation to alcohol drinking and delinquent behaviour among secondary school students - ScienceDirect.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = open(path,'r')\n",
    "page = page.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataList = []\n",
    "for link in soup.find_all('title'):\n",
    "    dataList.append(link.text)\n",
    "for x in dataList:\n",
    "    print(x,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.find(\"h2\",{'class':\"publication-title u-h3\"}, id=\"publication-title\")\n",
    "line = soup.find(\"div\",{'class':\"text-xs\"})\n",
    "print(title.text)\n",
    "print(line.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authors\n",
    "Author = soup.find('div',{'class':\"author-group\"},id = \"author-group\")\n",
    "print(Author.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Highlights\n",
    "cdata = {}\n",
    "c  = 0\n",
    "for link in soup.find_all('p'):\n",
    "    cdata[c] = link\n",
    "    c = c+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in cdata.items():\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for script in soup([\"script\", \"style\",\"meta\"]): # remove all javascript and stylesheet code\n",
    "    script.extract()\n",
    "text = soup.get_text()\n",
    "lines = (line.strip() for line in text.splitlines())\n",
    "chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all('p'):\n",
    "    print(link.text,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keywords:\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentence = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.text import Text\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = [',', '.', \"?\",\"4\",\"2\",\"2019\",\"(\",\"0\",'2015','213-220Download','2019',')','B.V','9','2017','z','&','By','28','='] # Remove words\n",
    "stop_words.extend(newStopWords)\n",
    "filtered_sentence2 = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence2.append(w)\n",
    "\n",
    "#print(word_tokens)\n",
    "print(filtered_sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"\"\n",
    "images = soup.find_all(\"img\")\n",
    "for image in images:\n",
    "    print(image['src'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "from urllib.request import urlopen\n",
    "\n",
    "download_folder = \"Downloads\"\n",
    "\n",
    "if not os.path.exists(download_folder):\n",
    "    os.makedirs(download_folder)\n",
    "for each in images:\n",
    "    url = each.get('src')\n",
    "    data = urlopen(url)\n",
    "    with open(os.path.join(download_folder, os.path.basename(url)), \"wb\") as f:\n",
    "        f.write(data.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find(\"div\",{'class':\"Tables\"})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = table.findAll('a')\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " fit2 = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        fit2.append(w)\n",
    "print(fit2)\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "for w in fit2:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(fit2)\n",
    "print(fdist)\n",
    "print(fdist.hapaxes()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aText = Text(nltk.corpus.gutenberg.words(text))\n",
    "print(text.collocations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['has', '896', '2018', '19','26', '108', '43', '922',\".\",\"?\"]\n",
    "stopwords.extend(newStopWords)\n",
    "import matplotlib.pyplot as plt\n",
    "freqdistPlot\n",
    "fdist.plot(5, cumulative=True)\n",
    "fdist_no_punc_no_stopwords = nltk.FreqDist(dict((word, freq) for word, freq in fdist.items() if word not in stopwords and word.isalpha()))\n",
    "fdist_no_punc_no_stopwords.plot(5, cumulative=False, title=\"50 most common tokens (no stopwords or punctuation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
